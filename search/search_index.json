{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome and about me I'm Jarek Gaze, and I'm working in IT since 2002 and below blog is for ordering my technolgies knowladge to help in every day work or share with colegues how we can some things do. My LinkedIn My Nofluffjobs Skill matrix Below list technologies and my level (in my opinion) Technologies Level OS Linux Docker Developer Python Core Spark Core/SQL Ansible Bash/AWK SQL PHP5-Core Bitbucket/GIT Databases Oracle/MYSQL PostgreSQL Hadoop HDP/CDP HDFS/YARN Hive Cloud GCP Interesting technologies Scala Java Spark Streaming","title":"Home"},{"location":"#welcome-and-about-me","text":"I'm Jarek Gaze, and I'm working in IT since 2002 and below blog is for ordering my technolgies knowladge to help in every day work or share with colegues how we can some things do. My LinkedIn My Nofluffjobs","title":"Welcome and about me"},{"location":"#skill-matrix","text":"Below list technologies and my level (in my opinion) Technologies Level OS Linux Docker Developer Python Core Spark Core/SQL Ansible Bash/AWK SQL PHP5-Core Bitbucket/GIT Databases Oracle/MYSQL PostgreSQL Hadoop HDP/CDP HDFS/YARN Hive Cloud GCP","title":"Skill matrix"},{"location":"#interesting-technologies","text":"Scala Java Spark Streaming","title":"Interesting technologies"},{"location":"markdown/","text":"Install and configure Install prerequirment software add-apt-repository ppa:git-core/ppa sudo apt-get update sudo apt-get install ca-certificates curl gnupg lsb-release sudo apt-get install docker-ce docker-ce-cli containerd.io sudo apt-get install docker-ce docker-ce-cli containerd.io sudo apt install apt-transport-https ca-certificates curl software-properties-common sudo apt-get update sudo apt-get update && sudo apt-get upgrade sudo apt-get install docker-ce docker-ce-cli containerd.io sudo apt-get install pyhton3 sudo apt install python3-pip Install mkdocs sudo apt install mkdocs mkdocs new . pip install -e mkdocs-material git submodule add https://github.com/squidfunk/mkdocs-material.git mkdocs-material # run in docker sudo docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material mkdocs build mkdocs serve mkdocs serve --livereload mkdocs gh-deploy --force Plugin install pip install git+https://github.com/jldiaz/mkdocs-plugin-tags.git pip install mkdocs-minify-plugin>=0.3.0 pip install mkdocs-awesome-pages-plugin>=2.5.0 pip install mkdocs-macros-plugin>=0.5.0 pip install mkdocs-redirects>=1.0.1 pip install mkdocs-git-revision-date-plugin>=0.3.1 pip install testresources pip install mkdocs-git-revision-date-plugin>=0.3.1 pip install mkdocs-git-revision-date-localized-plugin>=0.8 pip install tags pip install mkdocs-git-revision-date-plugin && pip install mkdocs-minify-plugin Usefull link (1) Official page (2) Basic syntax (3) Github page (4) Mardown reference PL (5) MkDocs (6) MkDocs Github (7) Mkdocs navigation Usefull Bootstrap icons Empty star Half star Full star","title":"Markdown"},{"location":"markdown/#install-and-configure","text":"","title":"Install and configure"},{"location":"markdown/#install-prerequirment-software","text":"add-apt-repository ppa:git-core/ppa sudo apt-get update sudo apt-get install ca-certificates curl gnupg lsb-release sudo apt-get install docker-ce docker-ce-cli containerd.io sudo apt-get install docker-ce docker-ce-cli containerd.io sudo apt install apt-transport-https ca-certificates curl software-properties-common sudo apt-get update sudo apt-get update && sudo apt-get upgrade sudo apt-get install docker-ce docker-ce-cli containerd.io sudo apt-get install pyhton3 sudo apt install python3-pip","title":"Install prerequirment software"},{"location":"markdown/#install-mkdocs","text":"sudo apt install mkdocs mkdocs new . pip install -e mkdocs-material git submodule add https://github.com/squidfunk/mkdocs-material.git mkdocs-material # run in docker sudo docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material mkdocs build mkdocs serve mkdocs serve --livereload mkdocs gh-deploy --force","title":"Install mkdocs"},{"location":"markdown/#plugin-install","text":"pip install git+https://github.com/jldiaz/mkdocs-plugin-tags.git pip install mkdocs-minify-plugin>=0.3.0 pip install mkdocs-awesome-pages-plugin>=2.5.0 pip install mkdocs-macros-plugin>=0.5.0 pip install mkdocs-redirects>=1.0.1 pip install mkdocs-git-revision-date-plugin>=0.3.1 pip install testresources pip install mkdocs-git-revision-date-plugin>=0.3.1 pip install mkdocs-git-revision-date-localized-plugin>=0.8 pip install tags pip install mkdocs-git-revision-date-plugin && pip install mkdocs-minify-plugin","title":"Plugin install"},{"location":"markdown/#usefull-link","text":"(1) Official page (2) Basic syntax (3) Github page (4) Mardown reference PL (5) MkDocs (6) MkDocs Github (7) Mkdocs navigation","title":"Usefull link"},{"location":"markdown/#usefull-bootstrap-icons","text":"Empty star Half star Full star","title":"Usefull Bootstrap icons"},{"location":"portfolio/","text":"Do\u015bwiadczenie zawodowe Firma Opis COMMERZBAK 2018-curently - Analiza folder\u00f3w HDFS w celu analizy po k\u0105tem problem\u00f3w z konfiguracj\u0105 HDP, HIVE (staging, compaction, multi versioning) (AWK) - HDP2GCP - system do backupu automatycznego z HDFS do GCP (PYTHON, HDP, GCP) - Ansible - tworzenie, modyfikacja skrypt\u00f3w instalacyjnych aktulaizacyjnych. Obecnie tworzenie skrypt\u00f3w dla CDP - - Standaryzacja nazewnictwa zmiennych i organizacja r\u00f3l. Normalizowanie proces\u00f3w w rolach ADD REMOVE TEST - SAS - Optymalizacja zada\u0144 HIVE SPARK ASSECO 2014-2018 - Optymalizacja zapyta\u0144 Oracle SQL - Tworzenie narz\u0119dzi monitoruj\u0105cych dzia\u0142anie system\u00f3w UNIX, LINUX, ORACLE dla ZABBIX - Oracle-BRM - SAS Orange 2002-2014 - Migrator - PLSQL migracja danych z jednego systemu ewidencyjnego do drugiego z dodatkow\u0105 walidacj\u0105 przesy\u0142anych danych. Odrzucone dane zosta\u0142y wyraportowane do klienta. - Sp\u0142yw \u2013 aplikacja sprawdzaj\u0105ca terminowo\u015b\u0107 I kompletno\u015b\u0107 plik\u00f3w otrzymanych z Pionu Sieci w systemie Kobat Kolektor (PHP+MySQL) - Automatyzacja zlece\u0144 \u2013 rejestracja zg\u0142aszanych problem\u00f3w w procesie rozliczania, raportowanie stanu i rozwi\u0105za\u0144 problem\u00f3w,generowanie skrypt\u00f3w wspomagaj\u0105cych analiz\u0119/wyja\u015bniania zlece\u0144 (PHP+MySQL) - Zarz\u0105dzanie Zwrotami \u2013 Wyja\u015bnianie i raportowanie przyczyn rekord\u00f3w zwracanych z system\u00f3w rozliczeniowych do systemu Kobat Kolektor (VB6) - 4 Sara CRT \u2013 aplikacja do rozliczania po\u0142\u0105cze\u0144 z telekonferencji automatycznych i p\u00f3\u0142automatycznych, rejestracja klient\u00f3w, generowanie raport\u00f3w i wk\u0142ad\u00f3w do faktur XLS, PDF, TXT. (PHP+MySQL+Ajax) - Przetwarzanie AWK bilingu w celu wystawienia faktur w systemie rozliczeniowym SERAT - Praca z UNIX, ORACLE, BASH, AWK, PHP Portale w mi\u0119dzyczasie Rok Firma Aktualny? Projekt Grafika Kodowanie W\u0142asny CMS SEO 2019 HANPAK 2018 ZKTECO 2018 NOWATEX COLLECTION 2017 DOBRA ENERGIA 2017 NOWATEX 2016 JAVOLODZ Prestashop 2016 DENTA FORTE 2015 IRBIS 2014 BIELPOL 2013 ARTOM 2013 BEJOT 2013 IGUATEC 2013 WEBER 2013 APSAUTOSERWIS 2012 Motorline 2009 CKM 2008 BOK W\u0142asny CMS - utworzony z wykorzystaniem PHP-Core i MySQL Modu\u0142y: * logowanie * administracja stronami - typy stron * edycja stron (pod k\u0105tem SEO) * strona zwyk\u0142a tekstowa * strona tekstowa z dodatkowymi elementami graficznymi * galeria zdj\u0119\u0107, link\u00f3w ze zdj\u0119ciami * definicja grup produkt\u00f3w, produkt\u00f3w i cech * translator - definiowanie s\u0142ownika powtarzaj\u0105ce si\u0119 zdania po angielsku + t\u0142umaczenie po polsku. Na podstawie s\u0142ownika ustawianie cech produktu w dw\u00f3ch wersjach jezykowych i tworzenie opis\u00f3w produkt\u00f3w automatycznie * definiowanie cech stron wy\u015bwietlaj\u0105cych listy produkt\u00f3w * wysy\u0142ka email ze strony Kontakt * modu\u0142 dost\u0119powy dla klienta w celu umo\u017cliwienia pobierania dokumentacji produkt\u00f3w Ponadto w PHP Core generowanie (4SaraCRT) * CSV * XLS * PDF","title":"Portfolio"},{"location":"portfolio/#doswiadczenie-zawodowe","text":"Firma Opis COMMERZBAK 2018-curently - Analiza folder\u00f3w HDFS w celu analizy po k\u0105tem problem\u00f3w z konfiguracj\u0105 HDP, HIVE (staging, compaction, multi versioning) (AWK) - HDP2GCP - system do backupu automatycznego z HDFS do GCP (PYTHON, HDP, GCP) - Ansible - tworzenie, modyfikacja skrypt\u00f3w instalacyjnych aktulaizacyjnych. Obecnie tworzenie skrypt\u00f3w dla CDP - - Standaryzacja nazewnictwa zmiennych i organizacja r\u00f3l. Normalizowanie proces\u00f3w w rolach ADD REMOVE TEST - SAS - Optymalizacja zada\u0144 HIVE SPARK ASSECO 2014-2018 - Optymalizacja zapyta\u0144 Oracle SQL - Tworzenie narz\u0119dzi monitoruj\u0105cych dzia\u0142anie system\u00f3w UNIX, LINUX, ORACLE dla ZABBIX - Oracle-BRM - SAS Orange 2002-2014 - Migrator - PLSQL migracja danych z jednego systemu ewidencyjnego do drugiego z dodatkow\u0105 walidacj\u0105 przesy\u0142anych danych. Odrzucone dane zosta\u0142y wyraportowane do klienta. - Sp\u0142yw \u2013 aplikacja sprawdzaj\u0105ca terminowo\u015b\u0107 I kompletno\u015b\u0107 plik\u00f3w otrzymanych z Pionu Sieci w systemie Kobat Kolektor (PHP+MySQL) - Automatyzacja zlece\u0144 \u2013 rejestracja zg\u0142aszanych problem\u00f3w w procesie rozliczania, raportowanie stanu i rozwi\u0105za\u0144 problem\u00f3w,generowanie skrypt\u00f3w wspomagaj\u0105cych analiz\u0119/wyja\u015bniania zlece\u0144 (PHP+MySQL) - Zarz\u0105dzanie Zwrotami \u2013 Wyja\u015bnianie i raportowanie przyczyn rekord\u00f3w zwracanych z system\u00f3w rozliczeniowych do systemu Kobat Kolektor (VB6) - 4 Sara CRT \u2013 aplikacja do rozliczania po\u0142\u0105cze\u0144 z telekonferencji automatycznych i p\u00f3\u0142automatycznych, rejestracja klient\u00f3w, generowanie raport\u00f3w i wk\u0142ad\u00f3w do faktur XLS, PDF, TXT. (PHP+MySQL+Ajax) - Przetwarzanie AWK bilingu w celu wystawienia faktur w systemie rozliczeniowym SERAT - Praca z UNIX, ORACLE, BASH, AWK, PHP","title":"Do\u015bwiadczenie zawodowe"},{"location":"portfolio/#portale-w-miedzyczasie","text":"Rok Firma Aktualny? Projekt Grafika Kodowanie W\u0142asny CMS SEO 2019 HANPAK 2018 ZKTECO 2018 NOWATEX COLLECTION 2017 DOBRA ENERGIA 2017 NOWATEX 2016 JAVOLODZ Prestashop 2016 DENTA FORTE 2015 IRBIS 2014 BIELPOL 2013 ARTOM 2013 BEJOT 2013 IGUATEC 2013 WEBER 2013 APSAUTOSERWIS 2012 Motorline 2009 CKM 2008 BOK W\u0142asny CMS - utworzony z wykorzystaniem PHP-Core i MySQL Modu\u0142y: * logowanie * administracja stronami - typy stron * edycja stron (pod k\u0105tem SEO) * strona zwyk\u0142a tekstowa * strona tekstowa z dodatkowymi elementami graficznymi * galeria zdj\u0119\u0107, link\u00f3w ze zdj\u0119ciami * definicja grup produkt\u00f3w, produkt\u00f3w i cech * translator - definiowanie s\u0142ownika powtarzaj\u0105ce si\u0119 zdania po angielsku + t\u0142umaczenie po polsku. Na podstawie s\u0142ownika ustawianie cech produktu w dw\u00f3ch wersjach jezykowych i tworzenie opis\u00f3w produkt\u00f3w automatycznie * definiowanie cech stron wy\u015bwietlaj\u0105cych listy produkt\u00f3w * wysy\u0142ka email ze strony Kontakt * modu\u0142 dost\u0119powy dla klienta w celu umo\u017cliwienia pobierania dokumentacji produkt\u00f3w Ponadto w PHP Core generowanie (4SaraCRT) * CSV * XLS * PDF","title":"Portale w mi\u0119dzyczasie"},{"location":"portfolioEn/","text":"Work experience Company Most important COMMERZBAK 2018-curently - Analysis of HDFS folders for looking for problems with HDP configuration, HIVE (staging, compaction, multi versioning) (AWK) - report in html, csv - HDP2GCP - automatic backup system from HDFS to GCP (PYTHON, HDP, GCP) - Ansible - creating, modifying installation installation scripts. Currently scripting for CDP. Standardization code - variable naming and role organization. - HIVE SPARK Quests Optimization ASSECO 2014-2018 - Optimalization SQL queries for Oracle-BRM reports - Create tools in bash/awk for monitoring Linux processes as input for ZABBIX - Create tools for analyzing SAS jobs for optimalization tasks Maintenace Orange 2002-2014 - Migrator - PLSQL data migration from one accounting system to another with validation of personal data. The rejected data has been reported to the client. - Rafting - an application that checks the timeliness and completeness of files received from the Network Division in the Kobat Kolektor system (PHP + MySQL) - Automation of orders - registration of payment settlement costs, status and obligations reporting, generation of ordered scripts / order clarification (PHP + MySQL) - Returns Management - contributing and reporting reasons for records returned from settlement systems to the Kobat Kolektor (VB6) system - 4 Sara CRT - an application for settling calls from automatic and semi-automatic teleconferences, customer registration, generating reports and payments for XLS, PDF, TXT invoices. (PHP + MySQL + Ajax) - AWK processing of billing to issue invoices in the SERAT billing system - Working with UNIX, ORACLE, BASH, AWK, PHP In meanwhile webpages Year Company ACTIVE Project Graphics Code MY CMS SEO 2019 HANPAK 2018 ZKTECO 2018 NOWATEX COLLECTION 2017 DOBRA ENERGIA 2017 NOWATEX 2016 JAVOLODZ Prestashop 2016 DENTA FORTE 2015 IRBIS 2014 BIELPOL 2013 ARTOM 2013 BEJOT 2013 IGUATEC 2013 WEBER 2013 APSAUTOSERWIS 2012 Motorline 2009 CKM 2008 BOK My CMS - created using PHP-Core and MySQL Modules: * login * site administration - page types/structure pages/subpages * website editing (for SEO) * plain text page * text page with additional graphics * photo gallery, photo links * definition of product groups, products and features * translator - defining a dictionary of repeated sentences in English + translation in Polish. Based on the dictionary, setting product features in two language versions and build description of products in automatically way * defining the features of pages displaying product lists * sending an email from the Contact page * customer access module to download product documentation Also in PHP Core, generating (4SaraCRT) * CSV * XLS * PDF","title":"Portfolio EN"},{"location":"portfolioEn/#work-experience","text":"Company Most important COMMERZBAK 2018-curently - Analysis of HDFS folders for looking for problems with HDP configuration, HIVE (staging, compaction, multi versioning) (AWK) - report in html, csv - HDP2GCP - automatic backup system from HDFS to GCP (PYTHON, HDP, GCP) - Ansible - creating, modifying installation installation scripts. Currently scripting for CDP. Standardization code - variable naming and role organization. - HIVE SPARK Quests Optimization ASSECO 2014-2018 - Optimalization SQL queries for Oracle-BRM reports - Create tools in bash/awk for monitoring Linux processes as input for ZABBIX - Create tools for analyzing SAS jobs for optimalization tasks Maintenace Orange 2002-2014 - Migrator - PLSQL data migration from one accounting system to another with validation of personal data. The rejected data has been reported to the client. - Rafting - an application that checks the timeliness and completeness of files received from the Network Division in the Kobat Kolektor system (PHP + MySQL) - Automation of orders - registration of payment settlement costs, status and obligations reporting, generation of ordered scripts / order clarification (PHP + MySQL) - Returns Management - contributing and reporting reasons for records returned from settlement systems to the Kobat Kolektor (VB6) system - 4 Sara CRT - an application for settling calls from automatic and semi-automatic teleconferences, customer registration, generating reports and payments for XLS, PDF, TXT invoices. (PHP + MySQL + Ajax) - AWK processing of billing to issue invoices in the SERAT billing system - Working with UNIX, ORACLE, BASH, AWK, PHP","title":"Work experience"},{"location":"portfolioEn/#in-meanwhile-webpages","text":"Year Company ACTIVE Project Graphics Code MY CMS SEO 2019 HANPAK 2018 ZKTECO 2018 NOWATEX COLLECTION 2017 DOBRA ENERGIA 2017 NOWATEX 2016 JAVOLODZ Prestashop 2016 DENTA FORTE 2015 IRBIS 2014 BIELPOL 2013 ARTOM 2013 BEJOT 2013 IGUATEC 2013 WEBER 2013 APSAUTOSERWIS 2012 Motorline 2009 CKM 2008 BOK My CMS - created using PHP-Core and MySQL Modules: * login * site administration - page types/structure pages/subpages * website editing (for SEO) * plain text page * text page with additional graphics * photo gallery, photo links * definition of product groups, products and features * translator - defining a dictionary of repeated sentences in English + translation in Polish. Based on the dictionary, setting product features in two language versions and build description of products in automatically way * defining the features of pages displaying product lists * sending an email from the Contact page * customer access module to download product documentation Also in PHP Core, generating (4SaraCRT) * CSV * XLS * PDF","title":"In meanwhile webpages"},{"location":"python/classes/","text":"Static and Dynamic fields class Astronaut: firstname = 'Mark' lastname = 'Watney' def __init__(self): self.firstname = 'Mark' self.lastname = 'Watney' @staticmethod Klasy statyczne - nie wymagaj\u0105 tworzenia obiektu class Uni: @staticmethod def hello(): print(\"Hello\") @classmethod Klasycznie class Student(object): def __init__(self, first_name, last_name): self.first_name = first_name self.last_name = last_name scott = Student('Scott', 'Robinson') Mo\u017cna zast\u0105pi\u0107 classmethod class Student(object): # Constructor removed for brevity @classmethod def from_string(cls, name_str): first_name, last_name = map(str, name_str.split(' ')) student = cls(first_name, last_name) return student @classmethod def from_json(cls, json_obj): # parse json... return student @classmethod def from_pickle(cls, pickle_file): # load pickle file... return student @dataclass fields may optionally specify a default value, using normal Python syntax: @dataclass class C: a: int # 'a' has no default value b: int = 0 # assign a default value for 'b' In this example, both a and b will be included in the added init () method, which will be defined as: def __init__(self, a: int, b: int = 0): TypeError will be raised if a field without a default value follows a field with a default value. This is true whether this occurs in a single class, or as a result of class inheritance.","title":"Classes"},{"location":"python/classes/#static-and-dynamic-fields","text":"class Astronaut: firstname = 'Mark' lastname = 'Watney' def __init__(self): self.firstname = 'Mark' self.lastname = 'Watney'","title":"Static and Dynamic fields"},{"location":"python/classes/#staticmethod","text":"Klasy statyczne - nie wymagaj\u0105 tworzenia obiektu class Uni: @staticmethod def hello(): print(\"Hello\")","title":"@staticmethod"},{"location":"python/classes/#classmethod","text":"Klasycznie class Student(object): def __init__(self, first_name, last_name): self.first_name = first_name self.last_name = last_name scott = Student('Scott', 'Robinson') Mo\u017cna zast\u0105pi\u0107 classmethod class Student(object): # Constructor removed for brevity @classmethod def from_string(cls, name_str): first_name, last_name = map(str, name_str.split(' ')) student = cls(first_name, last_name) return student @classmethod def from_json(cls, json_obj): # parse json... return student @classmethod def from_pickle(cls, pickle_file): # load pickle file... return student","title":"@classmethod"},{"location":"python/classes/#dataclass","text":"fields may optionally specify a default value, using normal Python syntax: @dataclass class C: a: int # 'a' has no default value b: int = 0 # assign a default value for 'b' In this example, both a and b will be included in the added init () method, which will be defined as: def __init__(self, a: int, b: int = 0): TypeError will be raised if a field without a default value follows a field with a default value. This is true whether this occurs in a single class, or as a result of class inheritance.","title":"@dataclass"},{"location":"python/dask/","text":"DASK Official page","title":"Dask"},{"location":"python/dask/#dask","text":"Official page","title":"DASK"},{"location":"python/examples/","text":"JIRA from jira import JIRA options = { \"server\": \"https://jira.address/\", \"basic_auth\": (\"USER\", \"PASSWORD\"), \"verify\": False } jira = JIRA(options) python -m pip install --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=filespythonhosted.org --proxy=https://GF2GAZE:Kn0w1ed9e&*7@140.100.200.9:8080 jira python -m pip install --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=filespythonhosted.org --proxy=https://GF2XXXX:PASSWORD@140.100.200.9:8080 jira In url password we need set for special chars using below notation https://de.wikipedia.org/wiki/URL-Encoding \u2423 ! \" # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] { | } %20 %21 %22 %23 %24 %25 %26 %27 %28 %29 %2A %2B %2C %2D %2E %2F %3A %3B %3C %3D %3E %3F %40 %5B %5C %5D %7B %7C %7D In windows we need configure PIP C:\\Apps\\pip\\pip.ini C:\\Users\\gf2gaze\\AppData\\Roaming\\pip pip.ini [global] trusted-host = pypi.python.org pypi.org files.pythonhosted.org Install JIRA package python -m pip install --user jira --proxy=https://GF2GAZE:Kn0w1ed9e%26%2A7@140.100.200.9:8080 python -m pip install --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=files.pythonhosted.org --proxy=https://USER:PASSWORD@IP.IP.IP.IP:8080 python -m pip install --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=files.pythonhosted.org --proxy=https://GF2GAZE:Proces%40%232@140.100.200.9:8080 jira","title":"Examples"},{"location":"python/examples/#jira","text":"from jira import JIRA options = { \"server\": \"https://jira.address/\", \"basic_auth\": (\"USER\", \"PASSWORD\"), \"verify\": False } jira = JIRA(options) python -m pip install --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=filespythonhosted.org --proxy=https://GF2GAZE:Kn0w1ed9e&*7@140.100.200.9:8080 jira python -m pip install --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=filespythonhosted.org --proxy=https://GF2XXXX:PASSWORD@140.100.200.9:8080 jira In url password we need set for special chars using below notation https://de.wikipedia.org/wiki/URL-Encoding \u2423 ! \" # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] { | } %20 %21 %22 %23 %24 %25 %26 %27 %28 %29 %2A %2B %2C %2D %2E %2F %3A %3B %3C %3D %3E %3F %40 %5B %5C %5D %7B %7C %7D In windows we need configure PIP C:\\Apps\\pip\\pip.ini C:\\Users\\gf2gaze\\AppData\\Roaming\\pip","title":"JIRA"},{"location":"python/examples/#pipini","text":"[global] trusted-host = pypi.python.org pypi.org files.pythonhosted.org Install JIRA package python -m pip install --user jira --proxy=https://GF2GAZE:Kn0w1ed9e%26%2A7@140.100.200.9:8080 python -m pip install --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=files.pythonhosted.org --proxy=https://USER:PASSWORD@IP.IP.IP.IP:8080 python -m pip install --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=files.pythonhosted.org --proxy=https://GF2GAZE:Proces%40%232@140.100.200.9:8080 jira","title":"pip.ini"},{"location":"python/install/","text":"Install Documentation Python Python PEP - propozycje ulepsze\u0144 Python PEP - PEP8 Python Time complexity Anaconda Jupyter Terraform Jupyter Display Jupyter version !py --version !python -V Themes and other intersting additions !pip install jupyterthemes !pip install --upgrade jupyterthemes !jt -t monokai # color schemes !jt -l HIVE CLIENT in PYTHON Hive client Python Ciekawa strona Matta MATT Matt Harasymczuk - Youtube Wiki Python Time Complexity TIME Complexity","title":"Install"},{"location":"python/install/#install","text":"","title":"Install"},{"location":"python/install/#documentation","text":"Python Python PEP - propozycje ulepsze\u0144 Python PEP - PEP8 Python Time complexity Anaconda Jupyter Terraform","title":"Documentation"},{"location":"python/install/#jupyter","text":"Display Jupyter version !py --version !python -V Themes and other intersting additions !pip install jupyterthemes !pip install --upgrade jupyterthemes !jt -t monokai # color schemes !jt -l HIVE CLIENT in PYTHON Hive client Python Ciekawa strona Matta MATT Matt Harasymczuk - Youtube Wiki Python Time Complexity TIME Complexity","title":"Jupyter"},{"location":"python/mut_im/","text":"Data Structures in Python Types and Sequences (Data Structures) int, float, complex, bool, NoneType str, bytes bytearray, list, tuple, set, frozenset, dict mutable immutable int float bool complex NoneType str bytearray bytes list tuples set frozenset dict mappingproxy array class dataclass dataclass(frozen=True) np.array pd.Series pd.DataFrame pd.SparseArray Python Arrays List: Used in JSON format Useful for Array operations Used in Databases Listy mog\u0105 zawiera\u0107 te same warto\u015bci wiele razy mo\u017cemy dodawa\u0107 elementy List = ['Milk', 'Bread', 'Eggs', 'Eggs', True, False, {}, (), ['a', 'b', 'c']] Tuple: Used to insert records in the database through SQL query at a time.Ex: (1.\u2019sravan\u2019, 34).(2.\u2019geek\u2019, 35) Used in parentheses checker Krotki s\u0105 sta\u0142e po definicji nie mo\u017cemy doda\u0107 nast\u0119pnych element\u00f3w Tuples = ('Milk', 'Bread', 'Eggs',) Set: Finding unique elements Join operations Jest to zbi\u00f3r danych kt\u00f3re nie mog\u0105 si\u0119 powtarza\u0107 Sets = {'Milk', 'Bread', 'Eggs'} String = \"Python for Everybody True\" Dictionary: Used to create a data frame with lists Used in JSON S\u0142owniki key-value pairs Dict = { 'Milk': 'Goats Milk', 'Eggs': 'Free Range Eggs', } Computational complexity list - O(n) tuple - O(n) set - O(1) dict - O(1) - in keys memory complexity cognitive complexity (and, or, not) branches (if, try, for, while) bytes for low level operations (sockets, files in binary mode) if you open a picture if transfer data over internet str - is unicode Singletons True False None you create an instance of a class dict data = dict() data = tuple() data = list() data = set() data = frozenset() data = str() data = int() data = float() syntactic sugar data = {} data = () data = [] data = '' data = 0 data = 0.0 from array import array data = array('B') from collections import namedtuple from dataclasses import dataclass from typing import NamedTuple, TypedDict identifier and a scalar -> value point1_x = 1 point1_y = 2 point1_z = 3 point2_x = 4 point2_y = 5 point2_z = 6 ## value and relation -> data point1 = (1, 2, 3) point2 = (4, 5, 6) ## data and contex -> information point1 = {'x': 1, 'y': 2, 'z': 3} point2 = {'x': 4, 'y': 5, 'z': 6} ## information and meaning -> classes class Point: x: int y: int z: int def show(self): return self.x, self.y, self.z class Point(TypedDict): x: int y: int z: int ## class class Point: x: int y: int z: int point = Point() point.x = 1 point.y = 2 point.z = 3 ## init class Point: x: int y: int z: int def __init__(self, x: int, y: int, z: int): self.x = x self.y = y self.z = z ## dataclass @dataclass class Point: x: int y: int z: int ARC - Automatic Reference Counting","title":"Mutable Immutable"},{"location":"python/mut_im/#data-structures-in-python","text":"","title":"Data Structures in Python"},{"location":"python/mut_im/#types-and-sequences-data-structures","text":"int, float, complex, bool, NoneType str, bytes bytearray, list, tuple, set, frozenset, dict mutable immutable int float bool complex NoneType str bytearray bytes list tuples set frozenset dict mappingproxy array class dataclass dataclass(frozen=True) np.array pd.Series pd.DataFrame pd.SparseArray","title":"Types and Sequences (Data Structures)"},{"location":"python/mut_im/#python-arrays","text":"","title":"Python Arrays"},{"location":"python/mut_im/#list","text":"Used in JSON format Useful for Array operations Used in Databases Listy mog\u0105 zawiera\u0107 te same warto\u015bci wiele razy mo\u017cemy dodawa\u0107 elementy List = ['Milk', 'Bread', 'Eggs', 'Eggs', True, False, {}, (), ['a', 'b', 'c']]","title":"List:"},{"location":"python/mut_im/#tuple","text":"Used to insert records in the database through SQL query at a time.Ex: (1.\u2019sravan\u2019, 34).(2.\u2019geek\u2019, 35) Used in parentheses checker Krotki s\u0105 sta\u0142e po definicji nie mo\u017cemy doda\u0107 nast\u0119pnych element\u00f3w Tuples = ('Milk', 'Bread', 'Eggs',)","title":"Tuple:"},{"location":"python/mut_im/#set","text":"Finding unique elements Join operations Jest to zbi\u00f3r danych kt\u00f3re nie mog\u0105 si\u0119 powtarza\u0107 Sets = {'Milk', 'Bread', 'Eggs'} String = \"Python for Everybody True\"","title":"Set:"},{"location":"python/mut_im/#dictionary","text":"Used to create a data frame with lists Used in JSON S\u0142owniki key-value pairs Dict = { 'Milk': 'Goats Milk', 'Eggs': 'Free Range Eggs', }","title":"Dictionary:"},{"location":"python/mut_im/#computational-complexity","text":"list - O(n) tuple - O(n) set - O(1) dict - O(1) - in keys memory complexity cognitive complexity (and, or, not) branches (if, try, for, while) bytes for low level operations (sockets, files in binary mode) if you open a picture if transfer data over internet","title":"Computational complexity"},{"location":"python/mut_im/#str-is-unicode","text":"","title":"str - is unicode"},{"location":"python/mut_im/#singletons","text":"True False None","title":"Singletons"},{"location":"python/mut_im/#you-create-an-instance-of-a-class-dict","text":"data = dict() data = tuple() data = list() data = set() data = frozenset() data = str() data = int() data = float()","title":"you create an instance of a class dict"},{"location":"python/mut_im/#syntactic-sugar","text":"data = {} data = () data = [] data = '' data = 0 data = 0.0 from array import array data = array('B') from collections import namedtuple from dataclasses import dataclass from typing import NamedTuple, TypedDict","title":"syntactic sugar"},{"location":"python/mut_im/#identifier-and-a-scalar-value","text":"point1_x = 1 point1_y = 2 point1_z = 3 point2_x = 4 point2_y = 5 point2_z = 6 ## value and relation -> data point1 = (1, 2, 3) point2 = (4, 5, 6) ## data and contex -> information point1 = {'x': 1, 'y': 2, 'z': 3} point2 = {'x': 4, 'y': 5, 'z': 6} ## information and meaning -> classes class Point: x: int y: int z: int def show(self): return self.x, self.y, self.z class Point(TypedDict): x: int y: int z: int ## class class Point: x: int y: int z: int point = Point() point.x = 1 point.y = 2 point.z = 3 ## init class Point: x: int y: int z: int def __init__(self, x: int, y: int, z: int): self.x = x self.y = y self.z = z ## dataclass @dataclass class Point: x: int y: int z: int","title":"identifier and a scalar -&gt; value"},{"location":"python/mut_im/#arc-automatic-reference-counting","text":"","title":"ARC - Automatic Reference Counting"},{"location":"python/pandas/","text":"Pandas Library for Python to work with data in table format Documentation PANDAS Local Jupyter example 1 # examples in Jupyter Local Jupyter example 1 # examples in Jupyter Basics Import Panda library import Pandas","title":"Pandas"},{"location":"python/pandas/#pandas","text":"Library for Python to work with data in table format","title":"Pandas"},{"location":"python/pandas/#documentation","text":"PANDAS Local Jupyter example 1 # examples in Jupyter Local Jupyter example 1 # examples in Jupyter","title":"Documentation"},{"location":"python/pandas/#basics","text":"Import Panda library import Pandas","title":"Basics"},{"location":"python/project-in-python/","text":"Creating projects Youtube Matt Harasymczuk - Youtube Patoarchitekci Python documentation Python Language Python PEP Python discuss group CPython CPython - Comparing changes Sphinx - Python Documentation Generator Pycharm - integration with issue tracker DDD domain driven design - Youtube Domain Driven Design w praktyce - Krzysztof Muchewicz Agile Atmosphere 2013: \"Scaling Agile@Allegro\" Krzysztof D\u0105browski (Allegro Group) Agile - summary in Pictures Atlassian Python API wrapper Automation work with JIRA SonarQube - automatic review code SonarQube - docker Repos #27 Patologie Mikroserwis\u00f3w - Repozytoria Bitbucket - Yet Another Commit Checker","title":"Projects in Python"},{"location":"python/project-in-python/#creating-projects","text":"","title":"Creating projects"},{"location":"python/project-in-python/#youtube","text":"Matt Harasymczuk - Youtube Patoarchitekci","title":"Youtube"},{"location":"python/project-in-python/#python-documentation","text":"Python Language Python PEP Python discuss group CPython CPython - Comparing changes Sphinx - Python Documentation Generator Pycharm - integration with issue tracker","title":"Python documentation"},{"location":"python/project-in-python/#ddd","text":"domain driven design - Youtube Domain Driven Design w praktyce - Krzysztof Muchewicz","title":"DDD"},{"location":"python/project-in-python/#agile","text":"Atmosphere 2013: \"Scaling Agile@Allegro\" Krzysztof D\u0105browski (Allegro Group) Agile - summary in Pictures Atlassian Python API wrapper Automation work with JIRA SonarQube - automatic review code SonarQube - docker","title":"Agile"},{"location":"python/project-in-python/#repos","text":"#27 Patologie Mikroserwis\u00f3w - Repozytoria Bitbucket - Yet Another Commit Checker","title":"Repos"},{"location":"python/scikit-learn/","text":"Scikit-learn The library for machine learning in Python Documentation SCIKIT Python_2020-12-11_14-05-17.mp4","title":"Scikit-learn"},{"location":"python/scikit-learn/#scikit-learn","text":"The library for machine learning in Python","title":"Scikit-learn"},{"location":"python/scikit-learn/#documentation","text":"SCIKIT Python_2020-12-11_14-05-17.mp4","title":"Documentation"},{"location":"python/seaborn/","text":"Seaborn The best library for data visualisation in Python Documentation Seaborn official","title":"Seaborn"},{"location":"python/seaborn/#seaborn","text":"The best library for data visualisation in Python","title":"Seaborn"},{"location":"python/seaborn/#documentation","text":"Seaborn official","title":"Documentation"},{"location":"spark/install/","text":"Install Installations steps on below page https://spark.apache.org/docs/latest/ https://spark.apache.org/downloads.html Using spark via docker Docker Hub Spark Cluster Standalone Spark Apache Spark Docker uhopper/hadoop-spark Marcel Jan","title":"Install"},{"location":"spark/install/#install","text":"Installations steps on below page https://spark.apache.org/docs/latest/ https://spark.apache.org/downloads.html Using spark via docker Docker Hub Spark Cluster Standalone Spark Apache Spark Docker uhopper/hadoop-spark Marcel Jan","title":"Install"},{"location":"spark/optimalization/","text":"Parametry uruchomieniowe spark.ui.enabled: \"true\" - pwoinno sie uruchamiac sporadycznie tylko do testow lub debug spark.executor.instances: 4 - liczba executorow do wykonania zadania , ilosc zbiorow (cpu i pamieci) spark.executor.cores: 4 spark.executor.memory: \"4G\" spark.driver.memory: \"8G\" Jezli mamy 32 taski i 16 core = 2 cykle zostana wykonane Zmiana z 30GB na 60GB nic nie zmieni Tylko zmiana core z 4 na 8 w 4 intancjach daje 32 - szybciej sie juz nie da Jezeli w tym ukladzie 32 zadania 32 cory zaczynaj czkac to znaczy ze ilosc danych jest wieksza niz przydzielona pamiec Jezeli moj dataset na jednej partycji ma wiecej niz 4G - to zacznie swapowac spearowac - jak zacznie zapisywac na dysk to wtedy zwolni wykonanie I wtedy dokladamy RAMu zeby uniknac zrzucania danych na dysk Jezeli jest wysoka wartosc na driverze - tzn ze dokonujecie obliczenia na driverze a nie do tego driver sluzy Tzn ze cos robicie w pythonie co jest rozproszone Np jezeli w Pythonie uzywamy Pandas to dzialamy na driverze w tym przypadku od razu bedzie widac przez brak jobow 1 - zwiekszyc RAM lub przypisac szybsze procek 2 - przerzucic obliczenia na executory a nie drivery (a jezeli sie upierasz ze przeslanie tego executory zajmie wiecej czasu to ok Twoja sprawa jak jestes zadowolony) DOdatkowo przepelniona pamiec bedzie powodowala czestsze uruchamianie i czyszczenie GC Driver zwykle jest mniejszy niz executiry Jak w\u0142\u0105czy\u0107 wy\u017cszy poziom logowania pyspark Jacek SPark Query Jacek SPark Buckets ./bin/pyspark - uruchomienie interaktywnej wersji Na porcie 4040 tworzy si\u0119 UI Wydajno\u015b\u0107 to nie przegl\u0105danie kod\u00f3w \u017ar\u00f3d\u0142owych. OPtymalizacja aplikacji ostatnim krokiem powinno by\u0107 przegl\u0105danie kod\u00f3w \u017ar\u00f3d\u0142owych. W celu optymalizacji prosimy o to co widzi Spark. Jak zrobi\u0107 zrzut wszystkiego More > Monitoring > Viewing after the Fact - ustawiamy parametry spark.eventLog.enabled true spark.eventLog.dir hdfs://namenode/shared/spark-logs ./bin/pyspark -c spark.eventLog.enabled=true -c spark.eventLog.dir=/tmp/jobik (katalog musi istnie\u0107) W UI > Environment mamy spis wszystkich ustawie\u0144 dla JOBa Jak zaimportowa\u0107 ? Spark monitoring Zak\u0142adka Executors - wy\u015bwietlenie executor\u00f3w SQL - zapytanie SQL Wchodzimy do joba iw idzimy Graph wykonania - u g\u00f3ry s\u0105 wej\u015bcia danych FileScan - skanujemy plik o formacie text value#2 - schema Batched - wsadowe czy strumieniowe DataFilters - nie by\u0142o \u017cadnych filtr\u00f3w Format - Format danych wej\u015bciowych text Location - wykorzystwyane przez Sparka do indeksowania job\u00f3w - z plik\u00f3w hdfs lub HiveMetastore, jak uruchamiamy spark to tworzony jest zuborzony hivemetastore w katalogu spark-warehouse w lokalizacji sparka spark-warehouse PartitionFilters PushedFilters ReadSchema Je\u017celi jest u\u017cywany go\u0142y Pandas to wtedy nie wida\u0107 tego w UI Log DAGScheduler - byt planowanie naszych TaskSetManager 0.0 - pojedynczy task uruchomiony do realizacji zadania Analizy zaczynamy od SQL - JOBS jest o wiele bardziej zaawansowany spark.range(5).write.saveAsTable(\"cb\") - zapytanie z 5 wierszowego DatFramu stworzy\u0142 tabele i zapisa\u0142 j\u0105 do lokalnego hive metastore spark-warehouse/cb/part-00000-cba69d18-1a53-4c7e-bb9b-02f512bdf2f8-c000.snappy.parquet 297 spark-warehouse/cb/part-00003-cba69d18-1a53-4c7e-bb9b-02f512bdf2f8-c000.snappy.parquet 472 spark-warehouse/cb/part-00006-cba69d18-1a53-4c7e-bb9b-02f512bdf2f8-c000.snappy.parquet 472 spark-warehouse/cb/part-00012-cba69d18-1a53-4c7e-bb9b-02f512bdf2f8-c000.snappy.parquet 472 spark-warehouse/cb/part-00015-cba69d18-1a53-4c7e-bb9b-02f512bdf2f8-c000.snappy.parquet 472 5 wierszy -> 6 plik\u00f3w 1 plik pusty - optymalizator sparka stwierdzi\u0142 \u017ce zapisze to w 6 partycjach. 1 plik jest pusty Odczyt plik spark.read.parquet(\"spark-warehouse/cb/part-00000-cba69d18-1a53-4c7e-bb9b-02f512bdf2f8-c000.snappy.parquet\").show() Odczyt plik wyra\u017cenie regularne spark.o.parquet(\"spark-warehouse/cb/part-000[0-1][1-6]-cba69d18-1a53-4c7e-bb9b-02f512bdf2f8-c000.snappy.parquet\").show() Na podstawie pliku mo\u017cemy odczyta\u0107 oznaczenie operacji cba69d18-1a53-4c7e-bb9b-02f512bdf2f8 spark.table(\"cb\").show() from pyspark.sql.functions import input_file_name df.withColumn(\"filename\", input_file_name()) spark.table(\"cb\").withColumn(\"filename\", input_file_name()).show(100000, false) - wy\u0142\u0105czenie Domy\u015blny format plik\u00f3w Zebranie logow z dzialania Sparka /tmp/jabik/local-1639042398382 ./sbin/start-history-server.sh - uruchomienie history servera i zaladowanie poprzedniej sesji WholeStageCodegen - kod wygenerowany w JAVA Range w srodku - output rows | V CreateDataSourceTableAsSelectCOmmand Jak najedziemy na CreateDataSourceTableAsSelectCOmmand mamy informacje Execute CreateDataSourceTableAsSelectCOmmand `cb`.ErrorIfExists,[id] - argumenty ktre przyjmuje funkcja W Intelij Ctrl+o wyszukiwanie klas funckji w projekcie W kodzie s\u0105 funkcje tj Range kt\u00f3re maj\u0105 swoj\u0119 reprezentacje Range - logiczna RaneExec - fizyczna - ta kt\u00f3ra jest wykonywana doProduce - funkcja ktora produkuje dane ./bin/spark-shell spark.range(5).quertExecution.debug.codegen generowanie kod\u00f3w kt\u00f3re s\u0105 odpwoiedzzialne za generowanie kod\u00f3w Wchodzimy w joba - patrzymy dalej na DAG (DAG Visualisation) - Patrzymy w link Associated SQL Query: <1> - Patrzymy w link Completed jobs: <1> - Duration - Stages Kazde zapytanie SQL - sklada sie 0 lub wiecej JOBOW Kazdy job skalda sie z 1 lub wiecej stage (napewno 1) Przy operacjach shuffle bedzie wiecej stage conajmniej 2 Shuffle wystepuje przy group by, join kazda operacja ktora wymaga ulozenia danych A ka\u017cdy stage sk\u0142ada si\u0119 z Task\u00f3w - 1 lub wiele Taski s\u0105 najmniejszymi jednostkami kt\u00f3re odpowiadaj\u0105 za fizyczn\u0105 realizacj\u0119 wykonuj\u0105 fizycznie kod i przetwarzaj\u0105 fizycznie dane Wykonano 16 completed task - wykonywane na 8 rdzeniach czyli 16 vCorach Czas wykonania stage to jest najd\u0142u\u017cszy czas wykonania zdania Dlatego bardzo uwa\u017cnie patrzymy na statystyki Stage Wyliczenie 75 percentyla czasami = max - to wynikac moze z tego ze mamy ma\u0142\u0105 pr\u00f3bk\u0119 danych i dlatego statystyka nie odzwierciedla Mala wartosc MIN moze oznaczac ze mamy duzo pustych przebiegow Na 16 taskow mamy tylko 5 rekordow Dlaczego tak sie dzieje bo Spark tego nie wie i nie chce wiedziec. Nie chce ingerowac AQEShuffleRead - optymalizacja Shuffle read Analiza linika po linijce uruchamianie i sprawdzanie co sie dzieje ANaliza z formatki SQL wykonania kodu W scali pokazana jest dok\u0142adna nazwa funkcji show() w PySpark bedziemy mieli showString bo Python jest przerabiany na kod Scala Java Techniki Partycjonowanie - Jak ladowac dane do partycji w oparciu o wartosci w Oraclu mamy create partition YYYYMM jak to zrobic w Sparku. Proces Sparka dziala tymczasowo - Oracle caly czas. Na czas wykonania zadania mozemy sterowac iloscia partycji i podzialem danych potem nie mamy wplywu jezeli pojdzie na tym inny kod Bucketing - may gwarancje rownoleglego rozloenia danych BroadCastJoin - tabele faktow lub dimention - najszybszy join szybciej sie juz nie da Adaptive Query Execution - bardzo nowa rzecz","title":"Optimalization"},{"location":"spark/optimalization/#parametry-uruchomieniowe","text":"spark.ui.enabled: \"true\" - pwoinno sie uruchamiac sporadycznie tylko do testow lub debug spark.executor.instances: 4 - liczba executorow do wykonania zadania , ilosc zbiorow (cpu i pamieci) spark.executor.cores: 4 spark.executor.memory: \"4G\" spark.driver.memory: \"8G\" Jezli mamy 32 taski i 16 core = 2 cykle zostana wykonane Zmiana z 30GB na 60GB nic nie zmieni Tylko zmiana core z 4 na 8 w 4 intancjach daje 32 - szybciej sie juz nie da Jezeli w tym ukladzie 32 zadania 32 cory zaczynaj czkac to znaczy ze ilosc danych jest wieksza niz przydzielona pamiec Jezeli moj dataset na jednej partycji ma wiecej niz 4G - to zacznie swapowac spearowac - jak zacznie zapisywac na dysk to wtedy zwolni wykonanie I wtedy dokladamy RAMu zeby uniknac zrzucania danych na dysk Jezeli jest wysoka wartosc na driverze - tzn ze dokonujecie obliczenia na driverze a nie do tego driver sluzy Tzn ze cos robicie w pythonie co jest rozproszone Np jezeli w Pythonie uzywamy Pandas to dzialamy na driverze w tym przypadku od razu bedzie widac przez brak jobow 1 - zwiekszyc RAM lub przypisac szybsze procek 2 - przerzucic obliczenia na executory a nie drivery (a jezeli sie upierasz ze przeslanie tego executory zajmie wiecej czasu to ok Twoja sprawa jak jestes zadowolony) DOdatkowo przepelniona pamiec bedzie powodowala czestsze uruchamianie i czyszczenie GC Driver zwykle jest mniejszy niz executiry Jak w\u0142\u0105czy\u0107 wy\u017cszy poziom logowania pyspark Jacek SPark Query Jacek SPark Buckets ./bin/pyspark - uruchomienie interaktywnej wersji Na porcie 4040 tworzy si\u0119 UI Wydajno\u015b\u0107 to nie przegl\u0105danie kod\u00f3w \u017ar\u00f3d\u0142owych. OPtymalizacja aplikacji ostatnim krokiem powinno by\u0107 przegl\u0105danie kod\u00f3w \u017ar\u00f3d\u0142owych. W celu optymalizacji prosimy o to co widzi Spark. Jak zrobi\u0107 zrzut wszystkiego More > Monitoring > Viewing after the Fact - ustawiamy parametry spark.eventLog.enabled true spark.eventLog.dir hdfs://namenode/shared/spark-logs ./bin/pyspark -c spark.eventLog.enabled=true -c spark.eventLog.dir=/tmp/jobik (katalog musi istnie\u0107) W UI > Environment mamy spis wszystkich ustawie\u0144 dla JOBa","title":"Parametry uruchomieniowe"},{"location":"spark/optimalization/#jak-zaimportowac","text":"Spark monitoring Zak\u0142adka Executors - wy\u015bwietlenie executor\u00f3w SQL - zapytanie SQL Wchodzimy do joba iw idzimy Graph wykonania - u g\u00f3ry s\u0105 wej\u015bcia danych FileScan - skanujemy plik o formacie text value#2 - schema Batched - wsadowe czy strumieniowe DataFilters - nie by\u0142o \u017cadnych filtr\u00f3w Format - Format danych wej\u015bciowych text Location - wykorzystwyane przez Sparka do indeksowania job\u00f3w - z plik\u00f3w hdfs lub HiveMetastore, jak uruchamiamy spark to tworzony jest zuborzony hivemetastore w katalogu spark-warehouse w lokalizacji sparka spark-warehouse PartitionFilters PushedFilters ReadSchema Je\u017celi jest u\u017cywany go\u0142y Pandas to wtedy nie wida\u0107 tego w UI Log DAGScheduler - byt planowanie naszych TaskSetManager 0.0 - pojedynczy task uruchomiony do realizacji zadania Analizy zaczynamy od SQL - JOBS jest o wiele bardziej zaawansowany spark.range(5).write.saveAsTable(\"cb\") - zapytanie z 5 wierszowego DatFramu stworzy\u0142 tabele i zapisa\u0142 j\u0105 do lokalnego hive metastore spark-warehouse/cb/part-00000-cba69d18-1a53-4c7e-bb9b-02f512bdf2f8-c000.snappy.parquet 297 spark-warehouse/cb/part-00003-cba69d18-1a53-4c7e-bb9b-02f512bdf2f8-c000.snappy.parquet 472 spark-warehouse/cb/part-00006-cba69d18-1a53-4c7e-bb9b-02f512bdf2f8-c000.snappy.parquet 472 spark-warehouse/cb/part-00012-cba69d18-1a53-4c7e-bb9b-02f512bdf2f8-c000.snappy.parquet 472 spark-warehouse/cb/part-00015-cba69d18-1a53-4c7e-bb9b-02f512bdf2f8-c000.snappy.parquet 472 5 wierszy -> 6 plik\u00f3w 1 plik pusty - optymalizator sparka stwierdzi\u0142 \u017ce zapisze to w 6 partycjach. 1 plik jest pusty Odczyt plik spark.read.parquet(\"spark-warehouse/cb/part-00000-cba69d18-1a53-4c7e-bb9b-02f512bdf2f8-c000.snappy.parquet\").show() Odczyt plik wyra\u017cenie regularne spark.o.parquet(\"spark-warehouse/cb/part-000[0-1][1-6]-cba69d18-1a53-4c7e-bb9b-02f512bdf2f8-c000.snappy.parquet\").show() Na podstawie pliku mo\u017cemy odczyta\u0107 oznaczenie operacji cba69d18-1a53-4c7e-bb9b-02f512bdf2f8 spark.table(\"cb\").show() from pyspark.sql.functions import input_file_name df.withColumn(\"filename\", input_file_name()) spark.table(\"cb\").withColumn(\"filename\", input_file_name()).show(100000, false) - wy\u0142\u0105czenie Domy\u015blny format plik\u00f3w Zebranie logow z dzialania Sparka /tmp/jabik/local-1639042398382 ./sbin/start-history-server.sh - uruchomienie history servera i zaladowanie poprzedniej sesji WholeStageCodegen - kod wygenerowany w JAVA Range w srodku - output rows | V CreateDataSourceTableAsSelectCOmmand Jak najedziemy na CreateDataSourceTableAsSelectCOmmand mamy informacje Execute CreateDataSourceTableAsSelectCOmmand `cb`.ErrorIfExists,[id] - argumenty ktre przyjmuje funkcja W Intelij Ctrl+o wyszukiwanie klas funckji w projekcie W kodzie s\u0105 funkcje tj Range kt\u00f3re maj\u0105 swoj\u0119 reprezentacje Range - logiczna RaneExec - fizyczna - ta kt\u00f3ra jest wykonywana doProduce - funkcja ktora produkuje dane ./bin/spark-shell spark.range(5).quertExecution.debug.codegen generowanie kod\u00f3w kt\u00f3re s\u0105 odpwoiedzzialne za generowanie kod\u00f3w Wchodzimy w joba - patrzymy dalej na DAG (DAG Visualisation) - Patrzymy w link Associated SQL Query: <1> - Patrzymy w link Completed jobs: <1> - Duration - Stages Kazde zapytanie SQL - sklada sie 0 lub wiecej JOBOW Kazdy job skalda sie z 1 lub wiecej stage (napewno 1) Przy operacjach shuffle bedzie wiecej stage conajmniej 2 Shuffle wystepuje przy group by, join kazda operacja ktora wymaga ulozenia danych A ka\u017cdy stage sk\u0142ada si\u0119 z Task\u00f3w - 1 lub wiele Taski s\u0105 najmniejszymi jednostkami kt\u00f3re odpowiadaj\u0105 za fizyczn\u0105 realizacj\u0119 wykonuj\u0105 fizycznie kod i przetwarzaj\u0105 fizycznie dane Wykonano 16 completed task - wykonywane na 8 rdzeniach czyli 16 vCorach Czas wykonania stage to jest najd\u0142u\u017cszy czas wykonania zdania Dlatego bardzo uwa\u017cnie patrzymy na statystyki Stage Wyliczenie 75 percentyla czasami = max - to wynikac moze z tego ze mamy ma\u0142\u0105 pr\u00f3bk\u0119 danych i dlatego statystyka nie odzwierciedla Mala wartosc MIN moze oznaczac ze mamy duzo pustych przebiegow Na 16 taskow mamy tylko 5 rekordow Dlaczego tak sie dzieje bo Spark tego nie wie i nie chce wiedziec. Nie chce ingerowac AQEShuffleRead - optymalizacja Shuffle read Analiza linika po linijce uruchamianie i sprawdzanie co sie dzieje ANaliza z formatki SQL wykonania kodu W scali pokazana jest dok\u0142adna nazwa funkcji show() w PySpark bedziemy mieli showString bo Python jest przerabiany na kod Scala Java","title":"Jak zaimportowa\u0107 ?"},{"location":"spark/optimalization/#techniki","text":"Partycjonowanie - Jak ladowac dane do partycji w oparciu o wartosci w Oraclu mamy create partition YYYYMM jak to zrobic w Sparku. Proces Sparka dziala tymczasowo - Oracle caly czas. Na czas wykonania zadania mozemy sterowac iloscia partycji i podzialem danych potem nie mamy wplywu jezeli pojdzie na tym inny kod Bucketing - may gwarancje rownoleglego rozloenia danych BroadCastJoin - tabele faktow lub dimention - najszybszy join szybciej sie juz nie da Adaptive Query Execution - bardzo nowa rzecz","title":"Techniki"}]}